	In this project I am collecting the data from different representatives from both parties, democrats and republicans, extract the users they follow,  cluster them into communities  and analyze which relationship they have. Next I am classifying their tweets into two categories: republican tweets and democrats tweets. The steps followed are the following:

Collect.py
After collecting the friends from each account, I have analyzed how users are related: which are the friends most commonly followed, what is the friend overlap, and represent links between users by plotting a graph. 
Collect the most recent 500 tweets from each user and label them as republican or democrat, to use as testing data for the classify.py

Cluster.py 
Here I am performing community detection on the users collected in Collect.py and predicting future links between nodes. I am using Girvan-Newman algorithm for community detection, and Jaccard score coefficient for link recommendation. As a result, I obtain two clusters, one with very few nodes, usually one, and another one with the rest of the nodes. In this case, the cluster with lower nodes contain a single node, the 'Energy Department' twitter account. From this clustering we can deduce that the single node is a node very popular between the rest of the users. From the results we can observe that this makes sense, as the 'Energy Department' is an account that is going to be followed by both Republicans and Democrats.

Classify.py
I have used an external tweets dataset that contains 84502 tweets from 2018 labelled as democrat or republican. This data is used for training a Logistic Regression model that will classify the test tweets collected in Collect.py as republicans or democrats. The overall accuracy is very low. The accuracy I obtained in the first attempt was 0.574009 , and the maximum that I have been able to obtain is 0.652785. I know this is a low accuracy considering this is a binary classifier, but after varying several parameters of the model and trying different approaches I have arrived to the conclusion that differentiating political ideologies is not as simple as it seems, as both parties will be talking about the same topics in the same period of time, and therefore the tokens they are using are going to be alike and with similar frequency. These are some of the measures I have taken in order to improve the accuracy:

1. Increase the amount of training data: Initially I was using around 3000 tweets and obtained a low accuracy. So I decided to use a greater amount of training input data in order to improve the classifier. But interestingly, the accuracy lowered instead of increasing. I concluded from this that if the training data is too large, the results get blurred because of the greater amount of tokens to consider for classification.

2. Initially I developed the classifier considering only the frequency of each token, and the frequency of each token given its surrounding tokens. To improve the accuracy I have added additional features such as the lexicon features. Here, I have created two arrays, considering the most common words used by republicans and democrats respectively. When analyzing this I understood what was the main reason why the classifier was giving such a low accuracy: the set of words was very similar in both cases. They were often the same words, but maybe with a  non-significative higher percentage for one of the parties. This makes sense, as it is likely that  both parties are talking about same issues. If we made a similar classification for less correlated groups (such a football players and politicians) the results of this classification will be much more accurate. 
 Moreover, I want to point out that the accuracy did increase when including for each party, the names of candidates of the opposite party, or the name of the opposite party (such as including “Obama”  and “democrats” in the republicans list of tokens.  Moreover, the accuracies vary significantly depending on the twitter accounts selected.

3. A solution to improve this classifier would be to implement a deep learning algorithm using a neural network. This way, we would not consider only the tokens as a "bag of words" for classification, but also consider the relationship between them. We could use a word embedding technique such as word2vec in order to represent data as a vector space considering the position of the words. 



